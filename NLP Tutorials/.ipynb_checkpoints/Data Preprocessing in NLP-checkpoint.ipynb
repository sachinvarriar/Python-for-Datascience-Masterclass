{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e4adf05",
   "metadata": {},
   "source": [
    "This notebook illustrated preprocessing techniques used in NLP. All examples are based on the nltk(natural language toolkit) in python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0951047",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2c7ba6",
   "metadata": {},
   "source": [
    "Often the most important step in NLP and text analytics. Itâ€™s the process of breaking a stream of textual data into words, terms, sentences, symbols, or some other meaningful elements called tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51c73216",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sachi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf4022d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import (word_tokenize,\n",
    "                          sent_tokenize,\n",
    "                          TreebankWordTokenizer, \n",
    "                          TweetTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "811edc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence ðŸ¤– concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8538378d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence ðŸ¤– concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.', 'The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them.', 'The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.']\n"
     ]
    }
   ],
   "source": [
    "text_to_sentence = sent_tokenize(text)\n",
    "print(text_to_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa999e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', 'ðŸ¤–', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'language', ',', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', '.', 'The', 'goal', 'is', 'a', 'computer', 'capable', 'of', '``', 'understanding', \"''\", 'the', 'contents', 'of', 'documents', ',', 'including', 'the', 'contextual', 'nuances', 'of', 'the', 'language', 'within', 'them', '.', 'The', 'technology', 'can', 'then', 'accurately', 'extract', 'information', 'and', 'insights', 'contained', 'in', 'the', 'documents', 'as', 'well', 'as', 'categorize', 'and', 'organize', 'the', 'documents', 'themselves', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_word = word_tokenize(text)\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b318a4e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence ðŸ¤– concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_sentence[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c212996f",
   "metadata": {},
   "source": [
    "## Stop word removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cb1221",
   "metadata": {},
   "source": [
    "Stop words are words which are repetitive and donâ€™t hold any information. For example, words like â€“ {that these, below, is, are, etc.} donâ€™t provide any information, so they need to be removed from the text. NLTK comes preloaded with a dictionary of stopwords for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef5d61c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sachi\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'do', \"don't\", 'because', \"shan't\", 'during', 'her', 'here', 'each', \"mightn't\", 'shan', 'be', 'wasn', 'how', 'won', 'the', \"won't\", 'which', 'off', 'm', \"should've\", 'hadn', 'd', 'so', \"that'll\", 'between', 'its', 'too', \"she's\", 'down', 'it', \"you've\", 'mightn', 'until', \"you'd\", \"haven't\", 'him', 'by', 't', 'those', 're', 'aren', 'but', 'while', 'whom', 'this', 'now', 'ain', 'with', \"couldn't\", 'yourselves', 's', 'don', 'any', 'after', 'such', 'to', 'of', 'or', 'being', 'have', 'needn', 'my', 'yourself', 'can', 'hasn', 'there', 'again', 'is', \"doesn't\", 'myself', 'through', 'up', 'when', \"mustn't\", 'once', 'what', 'i', 'am', 'having', 'about', 'only', 'our', \"shouldn't\", \"isn't\", 'in', 'against', \"didn't\", 'out', 'shouldn', 'mustn', 'weren', \"you're\", 'if', 'will', 'couldn', \"it's\", 'doesn', 'has', 'that', 'll', 'your', 'these', 'they', 'them', 'should', 'ours', 'we', 'under', 'isn', 'just', 've', 'from', 'above', 'as', 'their', 'further', \"weren't\", 'all', 'into', 'had', 'some', 'very', 'was', 'y', 'ourselves', 'then', 'his', 'herself', 'yours', 'than', 'himself', 'not', 'over', 'me', 'before', 'nor', 'doing', 'a', \"you'll\", 'and', 'most', \"wouldn't\", 'been', 'own', 'does', 'at', 'she', 'on', \"aren't\", 'where', 'more', \"needn't\", 'wouldn', 'theirs', 'are', 'other', 'did', 'were', 'hers', 'who', 'you', \"hasn't\", 'ma', 'themselves', 'same', \"hadn't\", 'didn', 'no', 'an', 'why', 'haven', 'both', 'few', 'he', 'itself', \"wasn't\", 'below', 'for', 'o'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec3e31c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_text = []\n",
    "tokenized_word = word_tokenize(text)\n",
    "for each_word in tokenized_word:\n",
    "    if each_word not in stop_words:\n",
    "        filtered_text.append(each_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1a47775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxenized list with stop words: ['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', 'ðŸ¤–', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'language', ',', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', '.', 'The', 'goal', 'is', 'a', 'computer', 'capable', 'of', '``', 'understanding', \"''\", 'the', 'contents', 'of', 'documents', ',', 'including', 'the', 'contextual', 'nuances', 'of', 'the', 'language', 'within', 'them', '.', 'The', 'technology', 'can', 'then', 'accurately', 'extract', 'information', 'and', 'insights', 'contained', 'in', 'the', 'documents', 'as', 'well', 'as', 'categorize', 'and', 'organize', 'the', 'documents', 'themselves', '.']\n",
      "Toxenized list with out stop words: ['Natural', 'language', 'processing', '(', 'NLP', ')', 'subfield', 'linguistics', ',', 'computer', 'science', ',', 'artificial', 'intelligence', 'ðŸ¤–', 'concerned', 'interactions', 'computers', 'human', 'language', ',', 'particular', 'program', 'computers', 'process', 'analyze', 'large', 'amounts', 'natural', 'language', 'data', '.', 'The', 'goal', 'computer', 'capable', '``', 'understanding', \"''\", 'contents', 'documents', ',', 'including', 'contextual', 'nuances', 'language', 'within', '.', 'The', 'technology', 'accurately', 'extract', 'information', 'insights', 'contained', 'documents', 'well', 'categorize', 'organize', 'documents', '.']\n"
     ]
    }
   ],
   "source": [
    "print('Toxenized list with stop words: {}'.format(tokenized_word))\n",
    "print('Toxenized list with out stop words: {}'.format(filtered_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2f21e5",
   "metadata": {},
   "source": [
    "## Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73618d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 63 samples and 94 outcomes>\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "freq_dist_of_words = FreqDist(tokenized_word)\n",
    "print(freq_dist_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f19e364d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 6),\n",
       " ('of', 5),\n",
       " ('and', 5),\n",
       " ('language', 4),\n",
       " (',', 4),\n",
       " ('.', 3),\n",
       " ('documents', 3),\n",
       " ('is', 2),\n",
       " ('a', 2),\n",
       " ('computer', 2)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dist_of_words.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ecf07b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 48 samples and 61 outcomes>\n"
     ]
    }
   ],
   "source": [
    "freq_dist_of_words_cleaned = FreqDist(filtered_text)\n",
    "print(freq_dist_of_words_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "833f27cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('language', 4),\n",
       " (',', 4),\n",
       " ('.', 3),\n",
       " ('documents', 3),\n",
       " ('computer', 2),\n",
       " ('computers', 2),\n",
       " ('The', 2),\n",
       " ('Natural', 1),\n",
       " ('processing', 1),\n",
       " ('(', 1)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dist_of_words_cleaned.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85177cb7",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50b67bd",
   "metadata": {},
   "source": [
    "One of the techniques to reduce a word, most a verb to it's root form or stem is called Stemming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4cc12d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pstemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89ecf112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural--->natur\n",
      "language--->languag\n",
      "processing--->process\n",
      "(--->(\n",
      "NLP--->nlp\n",
      ")--->)\n",
      "subfield--->subfield\n",
      "linguistics--->linguist\n",
      ",--->,\n",
      "computer--->comput\n",
      "science--->scienc\n",
      ",--->,\n",
      "artificial--->artifici\n",
      "intelligence--->intellig\n",
      "ðŸ¤–--->ðŸ¤–\n",
      "concerned--->concern\n",
      "interactions--->interact\n",
      "computers--->comput\n",
      "human--->human\n",
      "language--->languag\n",
      ",--->,\n",
      "particular--->particular\n",
      "program--->program\n",
      "computers--->comput\n",
      "process--->process\n",
      "analyze--->analyz\n",
      "large--->larg\n",
      "amounts--->amount\n",
      "natural--->natur\n",
      "language--->languag\n",
      "data--->data\n",
      ".--->.\n",
      "The--->the\n",
      "goal--->goal\n",
      "computer--->comput\n",
      "capable--->capabl\n",
      "``--->``\n",
      "understanding--->understand\n",
      "''--->''\n",
      "contents--->content\n",
      "documents--->document\n",
      ",--->,\n",
      "including--->includ\n",
      "contextual--->contextu\n",
      "nuances--->nuanc\n",
      "language--->languag\n",
      "within--->within\n",
      ".--->.\n",
      "The--->the\n",
      "technology--->technolog\n",
      "accurately--->accur\n",
      "extract--->extract\n",
      "information--->inform\n",
      "insights--->insight\n",
      "contained--->contain\n",
      "documents--->document\n",
      "well--->well\n",
      "categorize--->categor\n",
      "organize--->organ\n",
      "documents--->document\n",
      ".--->.\n"
     ]
    }
   ],
   "source": [
    "for word in filtered_text:\n",
    "    print(word + '--->' + pstemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e543277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snow_stem = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca462c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural--->natur\n",
      "language--->languag\n",
      "processing--->process\n",
      "(--->(\n",
      "NLP--->nlp\n",
      ")--->)\n",
      "subfield--->subfield\n",
      "linguistics--->linguist\n",
      ",--->,\n",
      "computer--->comput\n",
      "science--->scienc\n",
      ",--->,\n",
      "artificial--->artifici\n",
      "intelligence--->intellig\n",
      "ðŸ¤–--->ðŸ¤–\n",
      "concerned--->concern\n",
      "interactions--->interact\n",
      "computers--->comput\n",
      "human--->human\n",
      "language--->languag\n",
      ",--->,\n",
      "particular--->particular\n",
      "program--->program\n",
      "computers--->comput\n",
      "process--->process\n",
      "analyze--->analyz\n",
      "large--->larg\n",
      "amounts--->amount\n",
      "natural--->natur\n",
      "language--->languag\n",
      "data--->data\n",
      ".--->.\n",
      "The--->the\n",
      "goal--->goal\n",
      "computer--->comput\n",
      "capable--->capabl\n",
      "``--->``\n",
      "understanding--->understand\n",
      "''--->''\n",
      "contents--->content\n",
      "documents--->document\n",
      ",--->,\n",
      "including--->includ\n",
      "contextual--->contextu\n",
      "nuances--->nuanc\n",
      "language--->languag\n",
      "within--->within\n",
      ".--->.\n",
      "The--->the\n",
      "technology--->technolog\n",
      "accurately--->accur\n",
      "extract--->extract\n",
      "information--->inform\n",
      "insights--->insight\n",
      "contained--->contain\n",
      "documents--->document\n",
      "well--->well\n",
      "categorize--->categor\n",
      "organize--->organ\n",
      "documents--->document\n",
      ".--->.\n"
     ]
    }
   ],
   "source": [
    "for word in filtered_text:\n",
    "    print(word + '--->' + snow_stem.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ad9f86",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f05c217",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2a6d0826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sachi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\sachi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\omw-1.4.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0edf5697",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1447faf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_words_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5dd522d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenized_word = word_tokenize(text1)\n",
    "for each_word in tokenized_word:\n",
    "    lem_word = lemmatizer.lemmatize(each_word)\n",
    "    lemmatized_words_list.append(lem_word)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f2b3d2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with Stop Words: ['He', 'was', 'running', 'and', 'eating', 'at', 'same', 'time', '.', 'He', 'has', 'bad', 'habit', 'of', 'swimming', 'after', 'playing', 'long', 'hours', 'in', 'the', 'Sun', '.']\n",
      "Lemmatized Words list ['He', 'wa', 'running', 'and', 'eating', 'at', 'same', 'time', '.', 'He', 'ha', 'bad', 'habit', 'of', 'swimming', 'after', 'playing', 'long', 'hour', 'in', 'the', 'Sun', '.']\n"
     ]
    }
   ],
   "source": [
    "print('Text with Stop Words: {}'.format(tokenized_word))\n",
    "print('Lemmatized Words list {}'.format(lemmatized_words_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "94412308",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_words_list = []\n",
    "for each_word in tokenized_word:\n",
    "    lem_word_v = lemmatizer.lemmatize(each_word, pos=\"v\")\n",
    "    lemmatized_words_list.append(lem_word_v)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "75d83d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words list ['He', 'be', 'run', 'and', 'eat', 'at', 'same', 'time', '.', 'He', 'have', 'bad', 'habit', 'of', 'swim', 'after', 'play', 'long', 'hours', 'in', 'the', 'Sun', '.']\n"
     ]
    }
   ],
   "source": [
    "print('Lemmatized Words list {}'.format(lemmatized_words_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd024478",
   "metadata": {},
   "source": [
    "## Parts Of Speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e956d8bb",
   "metadata": {},
   "source": [
    "PoS tagging is the process of tagging an input text with the part of speech for each word. It identifies if each word is a noun, pronoun, adjective, adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4fe64947",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\sachi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sachi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('universal_tagset')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8a3dee6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRON'),\n",
       " (\"'m\", 'VERB'),\n",
       " ('going', 'VERB'),\n",
       " ('to', 'PRT'),\n",
       " ('watch', 'VERB'),\n",
       " ('Silicon', 'NOUN'),\n",
       " ('Valley', 'NOUN'),\n",
       " ('on', 'ADP'),\n",
       " ('HBO', 'NOUN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I'm going to watch Silicon Valley on HBO.\"\n",
    "tokenized_word = word_tokenize(text)\n",
    "nltk.pos_tag(tokenized_word, tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd832d9f",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0cfc7835",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\sachi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\sachi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6078106d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPE WASHINGTON\n",
      "GPE New York\n",
      "PERSON Loretta E. Lynch\n",
      "GPE Brooklyn\n"
     ]
    }
   ],
   "source": [
    "text = \"WASHINGTON -- In the wake of a string of abuses by New York police officers in the 1990s, Loretta E. Lynch, the top federal prosecutor in Brooklyn, spoke forcefully about the pain of a broken trust that African-Americans felt and said the responsibility for repairing generations of miscommunication and mistrust fell to law enforcement.\"\n",
    "for sent in nltk.sent_tokenize(text):\n",
    "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "        if hasattr(chunk, 'label'):\n",
    "            print(chunk.label(), ' '.join(c[0] for c in chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241f5092",
   "metadata": {},
   "source": [
    "## Word Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4e94ca8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('good.n.01'), Synset('good.n.02'), Synset('good.n.03'), Synset('commodity.n.01'), Synset('good.a.01'), Synset('full.s.06'), Synset('good.a.03'), Synset('estimable.s.02'), Synset('beneficial.s.01'), Synset('good.s.06'), Synset('good.s.07'), Synset('adept.s.01'), Synset('good.s.09'), Synset('dear.s.02'), Synset('dependable.s.04'), Synset('good.s.12'), Synset('good.s.13'), Synset('effective.s.04'), Synset('good.s.15'), Synset('good.s.16'), Synset('good.s.17'), Synset('good.s.18'), Synset('good.s.19'), Synset('good.s.20'), Synset('good.s.21'), Synset('well.r.01'), Synset('thoroughly.r.02')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "synonym = wordnet.synsets(\"good\")\n",
    "print(synonym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2d801d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "having desirable or positive qualities especially those suitable for a thing specified\n"
     ]
    }
   ],
   "source": [
    "print(synonym[4].definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5199c7ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
