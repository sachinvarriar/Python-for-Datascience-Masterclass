{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e62ac8d8",
   "metadata": {},
   "source": [
    "This notebook illustrated preprocessing techniques used in NLP. All examples are based on the nltk(natural language toolkit) in python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6c2e6e",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1db8435",
   "metadata": {},
   "source": [
    "Often the most important step in NLP and text analytics. Itâ€™s the process of breaking a stream of textual data into words, terms, sentences, symbols, or some other meaningful elements called tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "eac8d142",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sachi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1cb55d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import (word_tokenize,\n",
    "                          sent_tokenize,\n",
    "                          TreebankWordTokenizer, \n",
    "                          TweetTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "56b6afc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence ðŸ¤– concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves. The students went to an NLP course. The student goes to an NLP course.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "5a0b3e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence ðŸ¤– concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.', 'The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them.', 'The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.', 'The students went to an NLP course.', 'The student goes to an NLP course.']\n"
     ]
    }
   ],
   "source": [
    "text_to_sentence = sent_tokenize(text)\n",
    "print(text_to_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ab9775ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them.'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_sentence[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "17d33b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', 'ðŸ¤–', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'language', ',', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', '.', 'The', 'goal', 'is', 'a', 'computer', 'capable', 'of', '``', 'understanding', \"''\", 'the', 'contents', 'of', 'documents', ',', 'including', 'the', 'contextual', 'nuances', 'of', 'the', 'language', 'within', 'them', '.', 'The', 'technology', 'can', 'then', 'accurately', 'extract', 'information', 'and', 'insights', 'contained', 'in', 'the', 'documents', 'as', 'well', 'as', 'categorize', 'and', 'organize', 'the', 'documents', 'themselves', '.', 'The', 'students', 'went', 'to', 'an', 'NLP', 'course', '.', 'The', 'student', 'goes', 'to', 'an', 'NLP', 'course', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_word = word_tokenize(text)\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "2713669e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence ðŸ¤– concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_sentence[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "372e06fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'language'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_word[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f1ed01",
   "metadata": {},
   "source": [
    "## Stop word removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30dbf10",
   "metadata": {},
   "source": [
    "Stop words are words which are repetitive and donâ€™t hold any information. For example, words like â€“ {that these, below, is, are, etc.} donâ€™t provide any information, so they need to be removed from the text. NLTK comes preloaded with a dictionary of stopwords for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4b392abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'do', \"don't\", 'because', \"shan't\", 'during', 'her', 'here', 'each', \"mightn't\", 'shan', 'be', 'wasn', 'how', 'won', 'the', \"won't\", 'which', 'off', 'm', \"should've\", 'hadn', 'd', 'so', \"that'll\", 'between', 'its', 'too', \"she's\", 'down', 'it', \"you've\", 'mightn', 'until', \"you'd\", \"haven't\", 'him', 'by', 't', 'those', 're', 'aren', 'but', 'while', 'whom', 'this', 'now', 'ain', 'with', \"couldn't\", 'yourselves', 's', 'don', 'any', 'after', 'such', 'to', 'of', 'or', 'being', 'have', 'needn', 'my', 'yourself', 'can', 'hasn', 'there', 'again', 'is', \"doesn't\", 'myself', 'through', 'up', 'when', \"mustn't\", 'once', 'what', 'i', 'am', 'having', 'about', 'only', 'our', \"shouldn't\", \"isn't\", 'in', 'against', \"didn't\", 'out', 'shouldn', 'mustn', 'weren', \"you're\", 'if', 'will', 'couldn', \"it's\", 'doesn', 'has', 'that', 'll', 'your', 'these', 'they', 'them', 'should', 'ours', 'we', 'under', 'isn', 'just', 've', 'from', 'above', 'as', 'their', 'further', \"weren't\", 'all', 'into', 'had', 'some', 'very', 'was', 'y', 'ourselves', 'then', 'his', 'herself', 'yours', 'than', 'himself', 'not', 'over', 'me', 'before', 'nor', 'doing', 'a', \"you'll\", 'and', 'most', \"wouldn't\", 'been', 'own', 'does', 'at', 'she', 'on', \"aren't\", 'where', 'more', \"needn't\", 'wouldn', 'theirs', 'are', 'other', 'did', 'were', 'hers', 'who', 'you', \"hasn't\", 'ma', 'themselves', 'same', \"hadn't\", 'didn', 'no', 'an', 'why', 'haven', 'both', 'few', 'he', 'itself', \"wasn't\", 'below', 'for', 'o'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sachi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "92abfd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_text = []\n",
    "tokenized_word = word_tokenize(text)\n",
    "for each_word in tokenized_word:\n",
    "    if each_word not in stop_words:\n",
    "        filtered_text.append(each_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6423372f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized list with stop words: ['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', 'ðŸ¤–', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'language', ',', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', '.', 'The', 'goal', 'is', 'a', 'computer', 'capable', 'of', '``', 'understanding', \"''\", 'the', 'contents', 'of', 'documents', ',', 'including', 'the', 'contextual', 'nuances', 'of', 'the', 'language', 'within', 'them', '.', 'The', 'technology', 'can', 'then', 'accurately', 'extract', 'information', 'and', 'insights', 'contained', 'in', 'the', 'documents', 'as', 'well', 'as', 'categorize', 'and', 'organize', 'the', 'documents', 'themselves', '.', 'The', 'students', 'went', 'to', 'an', 'NLP', 'course', '.', 'The', 'student', 'goes', 'to', 'an', 'NLP', 'course', '.']\n",
      "Tokenized list with out stop words: ['Natural', 'language', 'processing', '(', 'NLP', ')', 'subfield', 'linguistics', ',', 'computer', 'science', ',', 'artificial', 'intelligence', 'ðŸ¤–', 'concerned', 'interactions', 'computers', 'human', 'language', ',', 'particular', 'program', 'computers', 'process', 'analyze', 'large', 'amounts', 'natural', 'language', 'data', '.', 'The', 'goal', 'computer', 'capable', '``', 'understanding', \"''\", 'contents', 'documents', ',', 'including', 'contextual', 'nuances', 'language', 'within', '.', 'The', 'technology', 'accurately', 'extract', 'information', 'insights', 'contained', 'documents', 'well', 'categorize', 'organize', 'documents', '.', 'The', 'students', 'went', 'NLP', 'course', '.', 'The', 'student', 'goes', 'NLP', 'course', '.']\n"
     ]
    }
   ],
   "source": [
    "print('Tokenized list with stop words: {}'.format(tokenized_word))\n",
    "print('Tokenized list with out stop words: {}'.format(filtered_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f3fcca",
   "metadata": {},
   "source": [
    "## Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "894a5717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 69 samples and 110 outcomes>\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "freq_dist_of_words = FreqDist(tokenized_word)\n",
    "print(freq_dist_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "43b598af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 6),\n",
       " ('of', 5),\n",
       " ('and', 5),\n",
       " ('.', 5),\n",
       " ('language', 4),\n",
       " (',', 4),\n",
       " ('to', 4),\n",
       " ('The', 4),\n",
       " ('NLP', 3),\n",
       " ('documents', 3)]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dist_of_words.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "9e4baf42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 53 samples and 73 outcomes>\n"
     ]
    }
   ],
   "source": [
    "freq_dist_of_words_cleaned = FreqDist(filtered_text)\n",
    "print(freq_dist_of_words_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "5c3586ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 5),\n",
       " ('language', 4),\n",
       " (',', 4),\n",
       " ('The', 4),\n",
       " ('NLP', 3),\n",
       " ('documents', 3),\n",
       " ('computer', 2),\n",
       " ('computers', 2),\n",
       " ('course', 2),\n",
       " ('Natural', 1)]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dist_of_words_cleaned.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359b20f4",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e56c26e",
   "metadata": {},
   "source": [
    "One of the techniques to reduce a word, most a verb to it's root form or stem is called Stemming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "41008983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pstemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5fe1ba27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural--->natur\n",
      "language--->languag\n",
      "processing--->process\n",
      "(--->(\n",
      "NLP--->nlp\n",
      ")--->)\n",
      "subfield--->subfield\n",
      "linguistics--->linguist\n",
      ",--->,\n",
      "computer--->comput\n",
      "science--->scienc\n",
      ",--->,\n",
      "artificial--->artifici\n",
      "intelligence--->intellig\n",
      "ðŸ¤–--->ðŸ¤–\n",
      "concerned--->concern\n",
      "interactions--->interact\n",
      "computers--->comput\n",
      "human--->human\n",
      "language--->languag\n",
      ",--->,\n",
      "particular--->particular\n",
      "program--->program\n",
      "computers--->comput\n",
      "process--->process\n",
      "analyze--->analyz\n",
      "large--->larg\n",
      "amounts--->amount\n",
      "natural--->natur\n",
      "language--->languag\n",
      "data--->data\n",
      ".--->.\n",
      "The--->the\n",
      "goal--->goal\n",
      "computer--->comput\n",
      "capable--->capabl\n",
      "``--->``\n",
      "understanding--->understand\n",
      "''--->''\n",
      "contents--->content\n",
      "documents--->document\n",
      ",--->,\n",
      "including--->includ\n",
      "contextual--->contextu\n",
      "nuances--->nuanc\n",
      "language--->languag\n",
      "within--->within\n",
      ".--->.\n",
      "The--->the\n",
      "technology--->technolog\n",
      "accurately--->accur\n",
      "extract--->extract\n",
      "information--->inform\n",
      "insights--->insight\n",
      "contained--->contain\n",
      "documents--->document\n",
      "well--->well\n",
      "categorize--->categor\n",
      "organize--->organ\n",
      "documents--->document\n",
      ".--->.\n",
      "The--->the\n",
      "students--->student\n",
      "went--->went\n",
      "NLP--->nlp\n",
      "course--->cours\n",
      ".--->.\n",
      "The--->the\n",
      "student--->student\n",
      "goes--->goe\n",
      "NLP--->nlp\n",
      "course--->cours\n",
      ".--->.\n"
     ]
    }
   ],
   "source": [
    "for word in filtered_text:\n",
    "    print(word + '--->' + pstemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "0242d518",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snow_stem = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "1463cec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural--->natur\n",
      "language--->languag\n",
      "processing--->process\n",
      "(--->(\n",
      "NLP--->nlp\n",
      ")--->)\n",
      "subfield--->subfield\n",
      "linguistics--->linguist\n",
      ",--->,\n",
      "computer--->comput\n",
      "science--->scienc\n",
      ",--->,\n",
      "artificial--->artifici\n",
      "intelligence--->intellig\n",
      "ðŸ¤–--->ðŸ¤–\n",
      "concerned--->concern\n",
      "interactions--->interact\n",
      "computers--->comput\n",
      "human--->human\n",
      "language--->languag\n",
      ",--->,\n",
      "particular--->particular\n",
      "program--->program\n",
      "computers--->comput\n",
      "process--->process\n",
      "analyze--->analyz\n",
      "large--->larg\n",
      "amounts--->amount\n",
      "natural--->natur\n",
      "language--->languag\n",
      "data--->data\n",
      ".--->.\n",
      "The--->the\n",
      "goal--->goal\n",
      "computer--->comput\n",
      "capable--->capabl\n",
      "``--->``\n",
      "understanding--->understand\n",
      "''--->''\n",
      "contents--->content\n",
      "documents--->document\n",
      ",--->,\n",
      "including--->includ\n",
      "contextual--->contextu\n",
      "nuances--->nuanc\n",
      "language--->languag\n",
      "within--->within\n",
      ".--->.\n",
      "The--->the\n",
      "technology--->technolog\n",
      "accurately--->accur\n",
      "extract--->extract\n",
      "information--->inform\n",
      "insights--->insight\n",
      "contained--->contain\n",
      "documents--->document\n",
      "well--->well\n",
      "categorize--->categor\n",
      "organize--->organ\n",
      "documents--->document\n",
      ".--->.\n",
      "The--->the\n",
      "students--->student\n",
      "went--->went\n",
      "NLP--->nlp\n",
      "course--->cours\n",
      ".--->.\n",
      "The--->the\n",
      "student--->student\n",
      "goes--->goe\n",
      "NLP--->nlp\n",
      "course--->cours\n",
      ".--->.\n"
     ]
    }
   ],
   "source": [
    "for word in filtered_text:\n",
    "    print(word + '--->' + snow_stem.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cd0c77",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "9434b39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f86c02f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sachi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\sachi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "350b1329",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun. He went to the pool at 2pm\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "c129b2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_words_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "87884cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenized_word = word_tokenize(text1)\n",
    "for each_word in tokenized_word:\n",
    "    lem_word = lemmatizer.lemmatize(each_word)\n",
    "    lemmatized_words_list.append(lem_word)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "875525b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with Stop Words: ['He', 'was', 'running', 'and', 'eating', 'at', 'same', 'time', '.', 'He', 'has', 'bad', 'habit', 'of', 'swimming', 'after', 'playing', 'long', 'hours', 'in', 'the', 'Sun', '.', 'He', 'went', 'to', 'the', 'pool', 'at', '2pm']\n",
      "Lemmatized Words list ['He', 'wa', 'running', 'and', 'eating', 'at', 'same', 'time', '.', 'He', 'ha', 'bad', 'habit', 'of', 'swimming', 'after', 'playing', 'long', 'hour', 'in', 'the', 'Sun', '.', 'He', 'went', 'to', 'the', 'pool', 'at', '2pm']\n"
     ]
    }
   ],
   "source": [
    "print('Text with Stop Words: {}'.format(tokenized_word))\n",
    "print('Lemmatized Words list {}'.format(lemmatized_words_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "c9d5d69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_words_list = []\n",
    "for each_word in tokenized_word:\n",
    "    lem_word_v = lemmatizer.lemmatize(each_word, pos=\"v\")\n",
    "    lemmatized_words_list.append(lem_word_v)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "85c38fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words list ['He', 'be', 'run', 'and', 'eat', 'at', 'same', 'time', '.', 'He', 'have', 'bad', 'habit', 'of', 'swim', 'after', 'play', 'long', 'hours', 'in', 'the', 'Sun', '.', 'He', 'go', 'to', 'the', 'pool', 'at', '2pm']\n"
     ]
    }
   ],
   "source": [
    "print('Lemmatized Words list {}'.format(lemmatized_words_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdecdb4f",
   "metadata": {},
   "source": [
    "## Parts Of Speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf8a7a1",
   "metadata": {},
   "source": [
    "PoS tagging is the process of tagging an input text with the part of speech for each word. It identifies if each word is a noun, pronoun, adjective, adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "be0d25f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\sachi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sachi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('universal_tagset')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "43729334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRON'),\n",
       " (\"'m\", 'VERB'),\n",
       " ('going', 'VERB'),\n",
       " ('to', 'PRT'),\n",
       " ('watch', 'VERB'),\n",
       " ('Silicon-Valley', 'NOUN'),\n",
       " ('on', 'ADP'),\n",
       " ('HBO', 'NOUN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I'm going to watch Silicon-Valley on HBO.\"\n",
    "tokenized_word = word_tokenize(text)\n",
    "nltk.pos_tag(tokenized_word, tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501895bd",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7b9d072d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\sachi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\sachi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "11fd7ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPE WASHINGTON\n",
      "GPE New York\n",
      "PERSON Loretta E. Lynch\n",
      "GPE Brooklyn\n",
      "ORGANIZATION APPLE\n"
     ]
    }
   ],
   "source": [
    "text = \"WASHINGTON -- In the wake of a string of abuses by New York police officers in the 1990s, Loretta E. Lynch, the top federal prosecutor in Brooklyn, spoke forcefully about the pain of a broken trust that African-Americans felt and said the responsibility for repairing generations of miscommunication and mistrust fell to law enforcement. Today APPLE has policies tha are inclusive\"\n",
    "for sent in nltk.sent_tokenize(text):\n",
    "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "        if hasattr(chunk, 'label'):\n",
    "            print(chunk.label(), ' '.join(c[0] for c in chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b048cc5",
   "metadata": {},
   "source": [
    "## Word Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "6972c791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('good.n.01'), Synset('good.n.02'), Synset('good.n.03'), Synset('commodity.n.01'), Synset('good.a.01'), Synset('full.s.06'), Synset('good.a.03'), Synset('estimable.s.02'), Synset('beneficial.s.01'), Synset('good.s.06'), Synset('good.s.07'), Synset('adept.s.01'), Synset('good.s.09'), Synset('dear.s.02'), Synset('dependable.s.04'), Synset('good.s.12'), Synset('good.s.13'), Synset('effective.s.04'), Synset('good.s.15'), Synset('good.s.16'), Synset('good.s.17'), Synset('good.s.18'), Synset('good.s.19'), Synset('good.s.20'), Synset('good.s.21'), Synset('well.r.01'), Synset('thoroughly.r.02')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "synonym = wordnet.synsets(\"good\")\n",
    "print(synonym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "4f535e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "having the normally expected amount\n"
     ]
    }
   ],
   "source": [
    "print(synonym[5].definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8b63bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
